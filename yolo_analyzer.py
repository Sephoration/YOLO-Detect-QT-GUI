"""
YOLOç»Ÿä¸€åˆ†æå™¨
æ•´åˆä¸‰ä¸ªæ¨¡å¼ï¼šæ£€æµ‹ã€åˆ†ç±»ã€å…³é”®ç‚¹
ç›´æ¥è°ƒç”¨æ¨¡å‹å¯¹è±¡ï¼Œç¦æ­¢ä½¿ç”¨.predict()æ–¹æ³•
èŒè´£ï¼šåªè´Ÿè´£æ¨¡å‹æ¨ç†ã€æ•°æ®å¤„ç†å’Œæ ‡å‡†åŒ–ï¼Œä¸åŒ…å«ä»»ä½•æ¸²æŸ“é€»è¾‘
æ¸²æŸ“èŒè´£å·²å®Œå…¨ç§»äº¤ç»™ baseDetect.py
"""

# ä¿®æ­£å¯¼å…¥è¯­å¥ - åªåœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ ä¸€æ¬¡
import cv2
import numpy as np
import torch
import time
import json
import os
from pathlib import Path
from typing import Union, Dict, Any, List, Optional, Tuple
from ultralytics import YOLO
# ä¿®æ”¹å¯¼å…¥è¯­å¥ä»¥åŒ¹é…æ­£ç¡®çš„æ–‡ä»¶å
from baseDetect import BaseDetect


class UnifiedYOLO:
    """
    ç»Ÿä¸€YOLOå¤„ç†å™¨ - ä¸“æ³¨äºæ¨¡å‹æ¨ç†å’Œæ•°æ®å¤„ç†
    èŒè´£æ¸…å•ï¼š
    1. âœ… æ¨¡å‹åŠ è½½ä¸ç®¡ç†
    2. âœ… æ¨¡å‹ç±»å‹è¯†åˆ«
    3. âœ… æ¨¡å‹æ¨ç†è°ƒç”¨
    4. âœ… åŸå§‹æ•°æ®æå–
    5. âœ… æ•°æ®æ ‡å‡†åŒ–å¤„ç†
    6. âœ… ç»Ÿè®¡ä¿¡æ¯è®¡ç®—
    7. âœ… é…ç½®å‚æ•°ç®¡ç†
    8. âŒ ä¸è´Ÿè´£ä»»ä½•å¯è§†åŒ–æ¸²æŸ“
    """
    
    def __init__(self, model_path: str, mode: str = 'auto',
                 conf_threshold: float = 0.25, iou_threshold: float = 0.7,
                 warmup: bool = True, config_path: str = None):
        """
        åˆå§‹åŒ–YOLOå¤„ç†å™¨
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            mode: è¿è¡Œæ¨¡å¼ ('auto', 'detection', 'classification', 'pose', 'segmentation')
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            iou_threshold: IOUé˜ˆå€¼
            warmup: æ˜¯å¦å¯ç”¨æ¨¡å‹é¢„çƒ­
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        
        self.model_path = model_path
        self.mode = self._detect_mode(model_path) if mode == 'auto' else mode
        self.conf_threshold = conf_threshold
        self.iou_threshold = iou_threshold
        self.warmup = warmup
        self.config_path = config_path
        
        # è®¾å¤‡é€‰æ‹©
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # æ¨¡å‹å¯¹è±¡ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self.model = None
        self.model_info = {}
        self.warmed_up = False
        
        # æ¨¡å‹å…ƒæ•°æ®
        self.num_keypoints = 0
        self.keypoint_shape = None
        self.skeleton_connect = []
        
        # æ¨ç†å‚æ•°
        self.conf = conf_threshold
        self.iou = iou_threshold
        self.img_size = None  # åˆå§‹ä¸ºNoneï¼Œå°†ä»æ¨¡å‹è·å–çœŸå®å°ºå¯¸
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        self.config = self._load_config()
        
        # è®¾ç½®æ¨¡å¼å‚æ•°
        self._setup_mode_params()
        
        # æ–°å¢ï¼šæ¸²æŸ“å™¨
        self.renderer = BaseDetect()
        
        print(f"ğŸ§  YOLOå¤„ç†å™¨åˆå§‹åŒ– | æ¨¡å¼: {self.mode} | è®¾å¤‡: {self.device}")
        print(f"ğŸ“Š å‚æ•°é…ç½® | ç½®ä¿¡åº¦: {self.conf} | IOU: {self.iou}")
    
    def _detect_mode(self, model_path: str) -> str:
        """
        è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹ - åŸºäºæ–‡ä»¶åå…³é”®è¯
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            
        Returns:
            str: æ¨¡å‹æ¨¡å¼ ('detection', 'classification', 'pose', 'segmentation')
        """
        filename = Path(model_path).name.lower()
        
        # æ–‡ä»¶åå…³é”®è¯åŒ¹é…
        if 'cls' in filename or 'classify' in filename:
            return 'classification'
        elif 'pose' in filename or 'keypoint' in filename:
            return 'pose'
        elif 'seg' in filename:
            return 'segmentation'
        else:  # é»˜è®¤æ£€æµ‹æ¨¡å¼
            return 'detection'
    
    def _load_config(self) -> Dict[str, Any]:
        """
        åŠ è½½é…ç½®æ–‡ä»¶
        
        Returns:
            Dict: é…ç½®å­—å…¸ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›ç©ºå­—å…¸
        """
        config = {}
        
        # å¦‚æœæ²¡æœ‰æä¾›é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå°è¯•è‡ªåŠ¨æŸ¥æ‰¾
        if not self.config_path:
            model_dir = os.path.dirname(self.model_path)
            model_name = os.path.splitext(os.path.basename(self.model_path))[0]
            self.config_path = os.path.join(model_dir, f"{model_name}.json")
        
        # æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.config_path):
            print(f"ğŸ“ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°: {self.config_path}")
            return config
        
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_path}")
            
            # ä»é…ç½®ä¸­æå–å…³é”®ç‚¹ä¿¡æ¯
            if 'keypoints' in config:
                keypoint_config = config['keypoints']
                self.num_keypoints = keypoint_config.get('num_keypoints', 0)
                self.skeleton_connect = keypoint_config.get('skeleton', [])
                
        except Exception as e:
            print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        
        return config
    
    def _setup_mode_params(self):
        """
        æ ¹æ®æ¨¡å¼å’Œé…ç½®è®¾ç½®æ¨ç†å‚æ•°
        
        æ³¨æ„ï¼šè¿™é‡Œåªè®¾ç½®æ¨ç†å‚æ•°ï¼Œä¸è®¾ç½®æ¸²æŸ“å‚æ•°
        æ¸²æŸ“å‚æ•°åº”è¯¥åœ¨ baseDetect ä¸­è®¾ç½®
        """
        # ä»é…ç½®ä¸­è·å–å‚æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'inference' in self.config:
            inference_config = self.config['inference']
            self.conf = inference_config.get('conf_threshold', self.conf_threshold)
            self.iou = inference_config.get('iou_threshold', self.iou_threshold)
        
        # æ¨¡å¼ç‰¹å®šçš„å‚æ•°è°ƒæ•´
        if self.mode == 'classification':
            # åˆ†ç±»æ¨¡å‹çš„å°ºå¯¸å°†åœ¨åŠ è½½æ¨¡å‹åä»æ¨¡å‹æœ¬èº«è·å–
            self.iou = 0.45  # åˆ†ç±»ä¸éœ€è¦IOUï¼Œä½†è®¾ç½®ä¸€ä¸ªé»˜è®¤å€¼
        elif self.mode == 'pose':
            self.conf = max(self.conf, 0.3)  # å§¿æ€ä¼°è®¡éœ€è¦æ›´é«˜çš„ç½®ä¿¡åº¦
    
    def _get_inference_size(self, frame: np.ndarray) -> int:
        """
        æ™ºèƒ½è·å–æ¨ç†å°ºå¯¸
        ç­–ç•¥ï¼š
        1. ä¼˜å…ˆä½¿ç”¨æ¨¡å‹é¢„è®¾å°ºå¯¸ (self.img_size)
        2. å¦‚æœè¾“å…¥å›¾ç‰‡å°ï¼Œä½¿ç”¨å›¾ç‰‡çš„æœ€å¤§è¾¹
        3. ç¡®ä¿å°ºå¯¸æ˜¯32çš„å€æ•°ï¼ˆYOLOè¦æ±‚ï¼‰
        
        Args:
            frame: è¾“å…¥å›¾åƒ
            
        Returns:
            int: æ¨ç†å°ºå¯¸
        """
        if self.img_size is not None:
            # ä½¿ç”¨æ¨¡å‹é¢„è®¾å°ºå¯¸
            model_size = self.img_size
            
            # è·å–å›¾ç‰‡å°ºå¯¸
            h, w = frame.shape[:2]
            max_side = max(h, w)
            
            # å¦‚æœå›¾ç‰‡å¾ˆå°ï¼Œä½¿ç”¨å›¾ç‰‡çš„æœ€å¤§è¾¹ï¼ˆä½†ä¸è¶…è¿‡æ¨¡å‹é¢„è®¾å°ºå¯¸ï¼‰
            if max_side < model_size:
                # ç¡®ä¿æ˜¯32çš„å€æ•°
                smart_size = ((max_side + 31) // 32) * 32
                # é™åˆ¶æœ€å°å°ºå¯¸ä¸º160ï¼Œæœ€å¤§ä¸ºæ¨¡å‹é¢„è®¾å°ºå¯¸
                smart_size = max(160, min(smart_size, model_size))
                print(f"ğŸ“ æ™ºèƒ½è°ƒæ•´æ¨ç†å°ºå¯¸: {model_size} -> {smart_size} (å›¾ç‰‡å°ºå¯¸: {w}x{h})")
                return smart_size
            
            return model_size
        
        # é»˜è®¤å›é€€å€¼
        return 640
    
    # ==================== æ¨¡å‹ç®¡ç†æ–¹æ³• ====================
    
    def load_model(self) -> bool:
        """
        åŠ è½½æ¨¡å‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        
        Returns:
            bool: æ¨¡å‹åŠ è½½æ˜¯å¦æˆåŠŸ
        """
        if self.model is not None:
            return True
        
        try:
            model_name = Path(self.model_path).name
            print(f"ğŸ“¥ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
            
            # åŠ è½½YOLOæ¨¡å‹
            self.model = YOLO(self.model_path)
            self.model.to(self.device)
            
            # ================== æ ¸å¿ƒä¿®å¤ï¼šä½¿ç”¨æ¨¡å‹çœŸå®ä»»åŠ¡ç±»å‹ ==================
            # ä¸è¦ä¿¡æ–‡ä»¶åï¼Œè¦ä¿¡æ¨¡å‹æœ¬èº«çš„ task å±æ€§
            if hasattr(self.model, 'task'):
                real_task = self.model.task  # è·å–çœŸå®ä»»åŠ¡ç±»å‹
                print(f"ğŸ” æ¨¡å‹è‡ªæŠ¥ä»»åŠ¡ç±»å‹: {real_task}")
                
                # å»ºç«‹æ˜ å°„å…³ç³» (YOLO task -> ç³»ç»Ÿ mode)
                task_map = {
                    'pose': 'pose',
                    'detect': 'detection',
                    'classify': 'classification',
                    'segment': 'segmentation'
                }
                
                # å¼ºåˆ¶æ›´æ–°æ¨¡å¼
                if real_task in task_map:
                    old_mode = self.mode
                    self.mode = task_map[real_task]
                    print(f"âœ… å·²è‡ªåŠ¨ä¿®æ­£è¿è¡Œæ¨¡å¼ä¸º: {self.mode}")
                    
                    # é‡æ–°è®¾ç½®æ¨¡å¼å‚æ•°
                    if old_mode != self.mode:
                        self._setup_mode_params()
                        print(f"ğŸ”§ å·²æ›´æ–°æ¨¡å¼ç‰¹å®šå‚æ•°")
            
            # ================== æ ¸å¿ƒä¿®å¤ï¼šè·å–æ¨¡å‹çœŸå®è¾“å…¥å°ºå¯¸ ==================
            if hasattr(self.model, 'imgsz'):
                model_imgsz = self.model.imgsz
                if isinstance(model_imgsz, (list, tuple)):
                    # å¦‚æœæ˜¯åˆ—è¡¨/å…ƒç»„ï¼Œå–ç¬¬ä¸€ä¸ªå€¼
                    self.img_size = model_imgsz[0]
                else:
                    # å¦‚æœæ˜¯å•ä¸ªå€¼ï¼Œç›´æ¥ä½¿ç”¨
                    self.img_size = model_imgsz
                
                print(f"ğŸ“ æ¨¡å‹é¢„è®¾è¾“å…¥å°ºå¯¸: {self.img_size}")
            else:
                # å›é€€åˆ°é»˜è®¤å€¼
                self.img_size = 640 if self.mode != 'classification' else 224
                print(f"ğŸ“ ä½¿ç”¨é»˜è®¤è¾“å…¥å°ºå¯¸: {self.img_size}")
            # ================== æ ¸å¿ƒä¿®å¤ç»“æŸ ==================
            
            # æ”¶é›†æ¨¡å‹å…ƒä¿¡æ¯
            self._collect_model_info()
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            print(f"ğŸ“‹ æ¨¡å‹ä¿¡æ¯ | ä»»åŠ¡: {self.model_info.get('task', 'unknown')} | "
                  f"ç±»åˆ«æ•°: {self.model_info.get('class_count', 0)}")
            
            # æ‰§è¡Œé¢„çƒ­
            if self.warmup and not self.warmed_up:
                self._perform_warmup()
                
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False
    
    def _collect_model_info(self):
        """
        ä»æ¨¡å‹æ”¶é›†å…ƒä¿¡æ¯
        
        æ³¨æ„ï¼šè¿™é‡Œåªæ”¶é›†æ¨¡å‹æœ¬èº«çš„ä¿¡æ¯ï¼Œä¸æ”¶é›†ä»»ä½•æ¸²æŸ“ç›¸å…³ä¿¡æ¯
        æ¸²æŸ“ç›¸å…³ä¿¡æ¯åº”è¯¥åœ¨ baseDetect ä¸­å¤„ç†
        """
        if self.model is None:
            return
        
        # åŸºç¡€æ¨¡å‹ä¿¡æ¯
        self.model_info = {
            'mode': self.mode,
            'device': self.device,
            'task': getattr(self.model, 'task', 'unknown'),
            'class_names': [],
            'class_count': 0,
            'input_size': self.img_size,  # ä½¿ç”¨çœŸå®çš„æ¨¡å‹è¾“å…¥å°ºå¯¸
        }
        
        try:
            # è·å–ç±»åˆ«ä¿¡æ¯
            if hasattr(self.model, 'names') and self.model.names:
                self.model_info['class_names'] = list(self.model.names.values())
                self.model_info['class_count'] = len(self.model.names)
            
            # è·å–å…³é”®ç‚¹ä¿¡æ¯ï¼ˆå¦‚æœæœ‰å…³é”®å…³é”®ç‚¹æ¨¡å‹ï¼‰
            if hasattr(self.model, 'nkpt') and self.model.nkpt:
                self.num_keypoints = self.model.nkpt
                self.model_info['num_keypoints'] = self.num_keypoints
            
            if hasattr(self.model, 'kpt_shape'):
                self.keypoint_shape = self.model.kpt_shape
                self.model_info['keypoint_shape'] = self.keypoint_shape
            
            # è·å–éª¨æ¶è¿æ¥ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
            if hasattr(self.model, 'skeleton'):
                self.skeleton_connect = self.model.skeleton
                self.model_info['skeleton'] = self.skeleton_connect
            
            print(f"ğŸ“Š æ¨¡å‹å…ƒä¿¡æ¯æ”¶é›†å®Œæˆ | å…³é”®ç‚¹: {self.num_keypoints} | "
                  f"è¾“å…¥å°ºå¯¸: {self.img_size}")
                
        except Exception as e:
            print(f"âš ï¸ æ”¶é›†æ¨¡å‹ä¿¡æ¯æ—¶å‡ºé”™: {e}")
    
    def _perform_warmup(self):
        """
        æ‰§è¡Œæ¨¡å‹é¢„çƒ­
        
        é¢„çƒ­å¯ä»¥å‡å°‘é¦–æ¬¡æ¨ç†çš„å»¶è¿Ÿï¼Œç‰¹åˆ«æ˜¯å¯¹äºGPUæ¨¡å‹
        """
        if self.model is None:
            return
            
        print(f"ğŸ”¥ å¼€å§‹æ¨¡å‹é¢„çƒ­ | è¾“å…¥å°ºå¯¸: {self.img_size}")
        start_time = time.time()
        
        try:
            # åˆ›å»ºè™šæ‹Ÿè¾“å…¥æ•°æ®
            dummy_input = np.random.randint(0, 255, 
                                          (self.img_size, self.img_size, 3), 
                                          dtype=np.uint8)
            
            with torch.no_grad():
                for i in range(3):
                    _ = self.model(
                        dummy_input, 
                        conf=self.conf, 
                        iou=self.iou, 
                        imgsz=self.img_size,  # ä½¿ç”¨æ¨¡å‹é¢„è®¾å°ºå¯¸é¢„çƒ­
                        verbose=False
                    )
            
            # æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            self.warmed_up = True
            warmup_time = (time.time() - start_time) * 1000
            print(f"âœ… æ¨¡å‹é¢„çƒ­å®Œæˆ | è€—æ—¶: {warmup_time:.2f}ms")
            
        except Exception as e:
            print(f"âš ï¸ æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}")
    
    # ==================== æ ¸å¿ƒæ•°æ®å¤„ç†æ–¹æ³• ====================
    
    def process_frame(self, frame: np.ndarray) -> Dict[str, Any]:
        """
        å¤„ç†å•å¸§å›¾åƒ - ä¸»å…¥å£æ–¹æ³•
        
        èŒè´£ï¼šåªå¤„ç†æ•°æ®ï¼Œä¸è¿›è¡Œä»»ä½•æ¸²æŸ“
        è¿”å›æ ‡å‡†åŒ–æ•°æ®ï¼Œä¾›æ¸²æŸ“å™¨ä½¿ç”¨
        
        Args:
            frame: è¾“å…¥å›¾åƒ (BGRæ ¼å¼)
            
        Returns:
            Dict: åŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸ï¼š
                - success: å¤„ç†æ˜¯å¦æˆåŠŸ
                - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœsuccess=Falseï¼‰
                - raw_image: åŸå§‹å›¾åƒå‰¯æœ¬
                - data_type: æ•°æ®ç±»å‹ ('detection','classification','pose','segmentation')
                - processed_data: æ ‡å‡†åŒ–åçš„å¤„ç†æ•°æ®
                - stats: ç»Ÿè®¡ä¿¡æ¯
                - model_info: æ¨¡å‹ä¿¡æ¯ï¼ˆä¾›æ¸²æŸ“å™¨å‚è€ƒï¼‰
        """
        start_time = time.time()
        
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        if not self.load_model():
            return self._create_error_result(frame, 'æ¨¡å‹åŠ è½½å¤±è´¥')
        
        try:
            # ============ å…³é”®ä¿®æ”¹ï¼šæ™ºèƒ½è·å–æ¨ç†å°ºå¯¸ ============
            inference_size = self._get_inference_size(frame)
            
            # æ ¹æ®æ¨¡å¼è°ƒç”¨ä¸åŒçš„æ•°æ®å¤„ç†æ–¹æ³•
            if self.mode == 'classification':
                result = self._process_classification_data(frame, inference_size)
            elif self.mode == 'pose':
                result = self._process_pose_data(frame, inference_size)
            elif self.mode == 'segmentation':
                result = self._process_segmentation_data(frame, inference_size)
            else:  # detection
                result = self._process_detection_data(frame, inference_size)
            
            # è®¡ç®—å¤„ç†æ—¶é—´
            inference_time = time.time() - start_time
            
            # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            result['stats']['inference_time'] = inference_time * 1000  # æ¯«ç§’
            result['stats']['fps'] = 1.0 / inference_time if inference_time > 0 else 0
            result['stats']['inference_size'] = inference_size  # è®°å½•å®é™…ä½¿ç”¨çš„æ¨ç†å°ºå¯¸
            
            # æ·»åŠ æ¨¡å‹ä¿¡æ¯
            result['model_info'] = self.model_info.copy()
            
            result['success'] = True
            return result
            
        except Exception as e:
            print(f"âŒ å¸§å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return self._create_error_result(frame, str(e))
    
    def _create_error_result(self, frame: np.ndarray, error_msg: str) -> Dict[str, Any]:
        """
        åˆ›å»ºé”™è¯¯ç»“æœ
        
        Args:
            frame: åŸå§‹å›¾åƒ
            error_msg: é”™è¯¯ä¿¡æ¯
            
        Returns:
            Dict: é”™è¯¯ç»“æœå­—å…¸
        """
        return {
            'success': False,
            'error': error_msg,
            'raw_image': frame.copy(),
            'data_type': self.mode,
            'processed_data': {},
            'stats': {
                'detection_count': 0,
                'avg_confidence': 0.0,
                'inference_time': 0,
                'fps': 0.0,
                'inference_size': self.img_size or 640
            },
            'model_info': self.model_info.copy()
        }
    
    def _process_detection_data(self, frame: np.ndarray, inference_size: int) -> Dict[str, Any]:
        """
        å¤„ç†ç›®æ ‡æ£€æµ‹æ•°æ®
        
        åªæå–å’Œæ ‡å‡†åŒ–æ•°æ®ï¼Œä¸è¿›è¡Œä»»ä½•æ¸²æŸ“
        
        Args:
            frame: è¾“å…¥å›¾åƒ
            inference_size: æ¨ç†æ—¶ä½¿ç”¨çš„å°ºå¯¸
            
        Returns:
            Dict: æ ‡å‡†åŒ–æ£€æµ‹æ•°æ®
        """
        with torch.no_grad():
            results = self.model(
                frame,
                conf=self.conf,
                iou=self.iou,
                imgsz=inference_size,  # ä½¿ç”¨æ™ºèƒ½è°ƒæ•´åçš„å°ºå¯¸
                verbose=False
            )
        
        result = results[0]
        
        # åˆå§‹åŒ–ç»“æœç»“æ„
        processed_data = {
            'detection': {
                'boxes': [],
                'labels': [],
                'confidences': [],
                'class_ids': []
            }
        }
        stats = {
            'detection_count': 0,
            'avg_confidence': 0.0,
            'class_distribution': {}
        }
        
        if result.boxes is None:
            return self._create_success_result(frame, 'detection', processed_data, stats)
        
        # æå–æ£€æµ‹ç»“æœ
        boxes = result.boxes.xyxy.cpu().numpy() if result.boxes.xyxy is not None else []
        confidences = result.boxes.conf.cpu().numpy() if result.boxes.conf is not None else []
        class_ids = result.boxes.cls.cpu().numpy().astype(int) if result.boxes.cls is not None else []
        
        # è·å–ç±»åˆ«åç§°
        class_names = []
        for cls_id in class_ids:
            if hasattr(result, 'names') and cls_id < len(result.names):
                class_names.append(result.names[cls_id])
            else:
                class_names.append(f"class_{cls_id}")
        
        # å¡«å……æ ‡å‡†åŒ–æ•°æ®
        processed_data['detection']['boxes'] = boxes.tolist() if len(boxes) > 0 else []
        processed_data['detection']['labels'] = class_names
        processed_data['detection']['confidences'] = confidences.tolist() if len(confidences) > 0 else []
        processed_data['detection']['class_ids'] = class_ids.tolist() if len(class_ids) > 0 else []
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        detection_count = len(boxes)
        avg_confidence = np.mean(confidences) if len(confidences) > 0 else 0.0
        
        # ç±»åˆ«åˆ†å¸ƒ
        class_distribution = {}
        for cls_name in class_names:
            class_distribution[cls_name] = class_distribution.get(cls_name, 0) + 1
        
        stats['detection_count'] = detection_count
        stats['avg_confidence'] = float(avg_confidence)
        stats['class_distribution'] = class_distribution
        
        return self._create_success_result(frame, 'detection', processed_data, stats)
    
    def _process_classification_data(self, frame: np.ndarray, inference_size: int) -> Dict[str, Any]:
        """
        å¤„ç†å›¾åƒåˆ†ç±»æ•°æ®
        
        Args:
            frame: è¾“å…¥å›¾åƒ
            inference_size: æ¨ç†æ—¶ä½¿ç”¨çš„å°ºå¯¸
            
        Returns:
            Dict: æ ‡å‡†åŒ–åˆ†ç±»æ•°æ®
        """
        with torch.no_grad():
            results = self.model(
                frame,
                conf=self.conf,
                imgsz=inference_size,  # ä½¿ç”¨æ™ºèƒ½è°ƒæ•´åçš„å°ºå¯¸
                verbose=False
            )
        
        result = results[0]
        
        # åˆå§‹åŒ–ç»“æœç»“æ„
        processed_data = {
            'classification': {
                'top_predictions': [],
                'all_probs': []
            }
        }
        stats = {
            'detection_count': 0,
            'avg_confidence': 0.0
        }
        
        if not hasattr(result, 'probs') or result.probs is None:
            return self._create_success_result(frame, 'classification', processed_data, stats)
        
        # è·å–æ¦‚ç‡å’Œç±»åˆ«
        probs = result.probs.data.cpu().numpy()
        
        # è·å–å‰3ä¸ªé¢„æµ‹ç»“æœï¼ˆæŒ‰ç½®ä¿¡åº¦é™åºï¼‰
        top_indices = np.argsort(probs)[-3:][::-1]
        top_probs = probs[top_indices]
        
        # è·å–ç±»åˆ«åç§°
        top_classes = []
        for idx in top_indices:
            if hasattr(result, 'names'):
                top_classes.append(result.names[idx])
            else:
                top_classes.append(f"class_{idx}")
        
        # å¡«å……æ ‡å‡†åŒ–æ•°æ®
        processed_data['classification']['top_predictions'] = [
            (cls_name, float(prob)) 
            for cls_name, prob in zip(top_classes, top_probs)
        ]
        processed_data['classification']['all_probs'] = probs.tolist()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats['detection_count'] = 1  # åˆ†ç±»ä»»åŠ¡å›ºå®šä¸º1
        stats['avg_confidence'] = float(top_probs[0])  # æœ€é«˜ç½®ä¿¡åº¦
        stats['top_class'] = top_classes[0]
        stats['top_confidence'] = float(top_probs[0])
        
        return self._create_success_result(frame, 'classification', processed_data, stats)
    
    def _process_pose_data(self, frame: np.ndarray, inference_size: int) -> Dict[str, Any]:
        """
        å¤„ç†å…³é”®ç‚¹æ£€æµ‹æ•°æ®
        
        Args:
            frame: è¾“å…¥å›¾åƒ
            inference_size: æ¨ç†æ—¶ä½¿ç”¨çš„å°ºå¯¸
            
        Returns:
            Dict: æ ‡å‡†åŒ–å…³é”®ç‚¹æ•°æ®
        """
        with torch.no_grad():
            results = self.model(
                frame,
                conf=self.conf,
                iou=self.iou,
                imgsz=inference_size,  # ä½¿ç”¨æ™ºèƒ½è°ƒæ•´åçš„å°ºå¯¸
                verbose=False
            )
        
        result = results[0]
        
        # åˆå§‹åŒ–ç»“æœç»“æ„
        processed_data = {
            'pose': {
                'boxes': [],
                'keypoints': [],
                'keypoints_conf': [],
                'skeleton_config': self.skeleton_connect
            }
        }
        stats = {
            'detection_count': 0,
            'avg_confidence': 0.0,
            'keypoint_count': 0,
            'num_people': 0
        }
        
        if result.boxes is None or result.keypoints is None:
            return self._create_success_result(frame, 'pose', processed_data, stats)
        
        # æå–æ£€æµ‹ç»“æœ
        boxes = result.boxes.xyxy.cpu().numpy() if result.boxes.xyxy is not None else []
        confidences = result.boxes.conf.cpu().numpy() if result.boxes.conf is not None else []
        keypoints = result.keypoints.xy.cpu().numpy() if result.keypoints.xy is not None else []
        keypoints_conf = result.keypoints.conf.cpu().numpy() if result.keypoints.conf is not None else []
        
        # å¡«å……æ ‡å‡†åŒ–æ•°æ®
        processed_data['pose']['boxes'] = boxes.tolist() if len(boxes) > 0 else []
        processed_data['pose']['keypoints'] = keypoints.tolist() if len(keypoints) > 0 else []
        processed_data['pose']['keypoints_conf'] = keypoints_conf.tolist() if len(keypoints_conf) > 0 else []
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        detection_count = len(boxes)
        avg_confidence = np.mean(confidences) if len(confidences) > 0 else 0.0
        
        # è®¡ç®—å¯è§å…³é”®ç‚¹æ•°é‡
        total_keypoints = 0
        for i in range(len(keypoints_conf)):
            if i < len(keypoints_conf):
                visible_keypoints = np.sum(keypoints_conf[i] > 0.1)
                total_keypoints += visible_keypoints
        
        stats['detection_count'] = detection_count
        stats['avg_confidence'] = float(avg_confidence)
        stats['keypoint_count'] = total_keypoints
        stats['num_people'] = len(boxes)
        
        return self._create_success_result(frame, 'pose', processed_data, stats)
    
    def _process_segmentation_data(self, frame: np.ndarray, inference_size: int) -> Dict[str, Any]:
        """
        å¤„ç†åˆ†å‰²æ£€æµ‹æ•°æ®
        
        Args:
            frame: è¾“å…¥å›¾åƒ
            inference_size: æ¨ç†æ—¶ä½¿ç”¨çš„å°ºå¯¸
            
        Returns:
            Dict: æ ‡å‡†åŒ–åˆ†å‰²æ•°æ®
        """
        with torch.no_grad():
            results = self.model(
                frame,
                conf=self.conf,
                iou=self.iou,
                imgsz=inference_size,  # ä½¿ç”¨æ™ºèƒ½è°ƒæ•´åçš„å°ºå¯¸
                verbose=False
            )
        
        result = results[0]
        
        # åˆå§‹åŒ–ç»“æœç»“æ„
        processed_data = {
            'segmentation': {
                'masks': [],
                'boxes': [],
                'class_ids': [],
                'confidences': []
            }
        }
        stats = {
            'detection_count': 0,
            'avg_confidence': 0.0,
            'class_distribution': {}
        }
        
        if result.masks is None:
            return self._create_success_result(frame, 'segmentation', processed_data, stats)
        
        # æå–åˆ†å‰²ç»“æœ
        masks = result.masks.data.cpu().numpy() if result.masks.data is not None else []
        boxes = result.boxes.xyxy.cpu().numpy() if result.boxes.xyxy is not None else []
        confidences = result.boxes.conf.cpu().numpy() if result.boxes.conf is not None else []
        class_ids = result.boxes.cls.cpu().numpy().astype(int) if result.boxes.cls is not None else []
        
        # å¡«å……æ ‡å‡†åŒ–æ•°æ®ï¼ˆæ³¨æ„ï¼šmaskså¯èƒ½æ˜¯å¤§æ•°ç»„ï¼Œè¿™é‡Œåªå­˜å‚¨å¼•ç”¨æˆ–è·¯å¾„ï¼‰
        processed_data['segmentation']['masks'] = masks.tolist() if len(masks) > 0 else []
        processed_data['segmentation']['boxes'] = boxes.tolist() if len(boxes) > 0 else []
        processed_data['segmentation']['class_ids'] = class_ids.tolist() if len(class_ids) > 0 else []
        processed_data['segmentation']['confidences'] = confidences.tolist() if len(confidences) > 0 else []
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        detection_count = len(masks)
        avg_confidence = np.mean(confidences) if len(confidences) > 0 else 0.0
        
        # ç±»åˆ«åˆ†å¸ƒ
        class_distribution = {}
        for cls_id in class_ids:
            if hasattr(result, 'names') and cls_id < len(result.names):
                cls_name = result.names[cls_id]
            else:
                cls_name = f"class_{cls_id}"
            class_distribution[cls_name] = class_distribution.get(cls_name, 0) + 1
        
        stats['detection_count'] = detection_count
        stats['avg_confidence'] = float(avg_confidence)
        stats['class_distribution'] = class_distribution
        
        return self._create_success_result(frame, 'segmentation', processed_data, stats)
    
    def _create_success_result(self, frame: np.ndarray, data_type: str, 
                         processed_data: Dict, stats: Dict) -> Dict[str, Any]:
        """
        åˆ›å»ºæˆåŠŸç»“æœ
        
        Args:
            frame: åŸå§‹å›¾åƒ
            data_type: æ•°æ®ç±»å‹
            processed_data: å¤„ç†åçš„æ•°æ®
            stats: ç»Ÿè®¡ä¿¡æ¯
            
        Returns:
            Dict: æˆåŠŸç»“æœå­—å…¸
        """
        return {
            'success': True,
            'error': None,
            'raw_image': frame.copy(),  # ç¡®ä¿è¿”å›åŸå§‹å›¾åƒ
            'data_type': data_type,
            'processed_data': processed_data,
            'stats': stats,
            'image': frame.copy()  # æ·»åŠ imageå­—æ®µä¾›æ¸²æŸ“å™¨ä½¿ç”¨
        }

    # å°†render_detectionæ–¹æ³•ç§»åˆ°ç±»å†…éƒ¨
    def render_detection(self, result: Dict[str, Any]) -> np.ndarray:
        """
        æ¸²æŸ“æ£€æµ‹ç»“æœ
        
        Args:
            result: process_frame è¿”å›çš„ç»“æœå­—å…¸
            
        Returns:
            np.ndarray: æ¸²æŸ“åçš„å›¾åƒ
        """
        if not result.get('success', False):
            # è¿”å›åŸå§‹å›¾åƒ
            return result.get('raw_image', np.zeros((100, 100, 3), dtype=np.uint8))
        
        # å‡†å¤‡æ¸²æŸ“å™¨æ‰€éœ€çš„æ•°æ®æ ¼å¼
        analyzer_result = {
            'success': True,
            'raw_image': result.get('raw_image'),
            'data_type': result.get('data_type', 'detection'),
            'processed_data': result.get('processed_data', {}),
            'stats': result.get('stats', {}),
            'model_info': result.get('model_info', {})
        }
        
        # ä½¿ç”¨ BaseDetect æ¸²æŸ“
        rendered_image = self.renderer.render(analyzer_result)
        
        # å¯é€‰ï¼šæ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        if 'stats' in result:
            rendered_image = self.renderer.draw_statistics(rendered_image, result['stats'])
        
        return rendered_image
    
    def update_params(self, conf_threshold=None, iou_threshold=None, img_size=None) -> bool:
        """
        å®æ—¶æ›´æ–°æ¨ç†å‚æ•°
        
        Args:
            conf_threshold: æ–°çš„ç½®ä¿¡åº¦é˜ˆå€¼ (0.0-1.0)
            iou_threshold: æ–°çš„IOUé˜ˆå€¼ (0.0-1.0)
            img_size: æ–°çš„è¾“å…¥å›¾åƒå°ºå¯¸
            
        Returns:
            bool: å‚æ•°æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        try:
            updated = False
            
            if conf_threshold is not None and 0.0 <= conf_threshold <= 1.0:
                self.conf = conf_threshold
                self.conf_threshold = conf_threshold
                print(f"ğŸ”„ ç½®ä¿¡åº¦é˜ˆå€¼æ›´æ–°ä¸º: {conf_threshold}")
                updated = True
            
            if iou_threshold is not None and 0.0 <= iou_threshold <= 1.0:
                self.iou = iou_threshold
                self.iou_threshold = iou_threshold
                print(f"ğŸ”„ IOUé˜ˆå€¼æ›´æ–°ä¸º: {iou_threshold}")
                updated = True
            
            if img_size is not None and img_size > 0:
                self.img_size = img_size
                print(f"ğŸ”„ è¾“å…¥å°ºå¯¸æ›´æ–°ä¸º: {img_size}")
                updated = True
            
            return updated
        except Exception as e:
            print(f"âŒ æ›´æ–°å‚æ•°æ—¶å‡ºé”™: {e}")
            return False
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        è·å–æ¨¡å‹ä¿¡æ¯
        
        Returns:
            Dict: æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        if not self.model_info:
            self._collect_model_info()
        
        return self.model_info.copy()
    
    def __call__(self, frame: np.ndarray) -> Dict[str, Any]:
        """
        ä½¿å¯¹è±¡å¯è°ƒç”¨
        
        Args:
            frame: è¾“å…¥å›¾åƒ
            
        Returns:
            Dict: å¤„ç†ç»“æœ
        """
        return self.process_frame(frame)
    
    # ==================== é™æ€æ–¹æ³• ====================
    
    @staticmethod
    def analyze_model_info(model_path: str) -> Dict[str, Any]:
        """
        é™æ€æ–¹æ³•ï¼šåˆ†ææ¨¡å‹ä¿¡æ¯ï¼ˆä¸çœŸæ­£åŠ è½½æ¨¡å‹ï¼‰
        
        ç”¨äºåœ¨åŠ è½½å‰é¢„è§ˆæ¨¡å‹ä¿¡æ¯
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            
        Returns:
            Dict: æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        try:
            import os
            from pathlib import Path
            
            filename = Path(model_path).name
            file_size = os.path.getsize(model_path)
            
            # æ ¹æ®æ–‡ä»¶åçŒœæµ‹æ¨¡å¼ï¼ˆä»…ä½œä¸º fallbackï¼‰
            if 'cls' in filename.lower() or 'classify' in filename.lower():
                fallback_task = 'classification'
                fallback_input_size = '224x224'
            elif 'pose' in filename.lower() or 'keypoint' in filename.lower():
                fallback_task = 'pose'
                fallback_input_size = '640x640'
            elif 'seg' in filename.lower():
                fallback_task = 'segmentation'
                fallback_input_size = '640x640'
            else:
                fallback_task = 'detection'
                fallback_input_size = '640x640'
            
            # å°è¯•è·å–æ›´å‡†ç¡®çš„æ¨¡å‹ä¿¡æ¯
            try:
                with torch.no_grad():
                    model = YOLO(model_path)
                    
                    # è·å–çœŸå®ä»»åŠ¡ç±»å‹
                    real_task = getattr(model, 'task', fallback_task)
                    
                    # å»ºç«‹æ˜ å°„å…³ç³» (YOLO task -> ç³»ç»Ÿ mode)
                    task_map = {
                        'pose': 'pose',
                        'detect': 'detection',
                        'classify': 'classification',
                        'segment': 'segmentation'
                    }
                    
                    # è½¬æ¢ä¸ºç³»ç»Ÿæ¨¡å¼
                    task_type = task_map.get(real_task, fallback_task)
                    
                    model_info = {
                        'model_name': filename,
                        'task': real_task,
                        'task_type': task_type,  # ç¡®ä¿è¿”å› task_type é”®
                        'class_names': list(model.names.values()) if hasattr(model, 'names') else [],
                        'class_count': len(model.names) if hasattr(model, 'names') else 0,
                        'input_size': getattr(model, 'imgsz', fallback_input_size),
                        'file_size': f"{file_size/1024/1024:.1f} MB"
                    }
                    
                    # è·å–å…³é”®ç‚¹ä¿¡æ¯
                    if hasattr(model, 'nkpt'):
                        model_info['num_keypoints'] = model.nkpt
                    
                    del model
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    
                    print(f"ğŸ” é™æ€åˆ†æ: æ¨¡å‹çœŸå®ä»»åŠ¡ç±»å‹: {real_task}, ç³»ç»Ÿæ¨¡å¼: {task_type}")
                    return model_info
                    
            except Exception as e:
                print(f"âš ï¸ è¯¦ç»†æ¨¡å‹ä¿¡æ¯è·å–å¤±è´¥: {e}")
                
                # è¿”å›åŸºæœ¬ä¿¡æ¯
                return {
                    'model_name': filename,
                    'task_type': fallback_task,
                    'input_size': fallback_input_size,
                    'file_size': f"{file_size/1024/1024:.1f} MB",
                    'class_count': 'æœªçŸ¥'
                }
                
        except Exception as e:
            print(f"âŒ æ¨¡å‹ä¿¡æ¯åˆ†æå¤±è´¥: {e}")
            return None